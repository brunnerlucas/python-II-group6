{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imp'></a>\n",
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T11:49:30.701996Z",
     "iopub.status.busy": "2023-01-03T11:49:30.70144Z",
     "iopub.status.idle": "2023-01-03T11:49:47.028779Z",
     "shell.execute_reply": "2023-01-03T11:49:47.027156Z",
     "shell.execute_reply.started": "2023-01-03T11:49:30.701905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arch\n",
      "  Downloading arch-7.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from arch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from arch) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from arch) (2.2.2)\n",
      "Requirement already satisfied: statsmodels>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from arch) (0.14.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4->arch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4->arch) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4->arch) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels>=0.12->arch) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels>=0.12->arch) (23.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels>=0.12->arch) (1.16.0)\n",
      "Downloading arch-7.2.0-cp312-cp312-macosx_11_0_arm64.whl (927 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.5/927.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: arch\n",
      "Successfully installed arch-7.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arch\n",
    "import math\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from arch import arch_model\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from ipywidgets import HBox, VBox\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import plot_importance, plot_tree\n",
    "#Matplotlib style\n",
    "plt.style.use('fivethirtyeight')\n",
    "#Ignoring some warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rea'></a>\n",
    "# 3. Reading the data and preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://www.kaggle.com/datasets/lucastrenzado/repsol-stock-data-20-years\n",
    "<br/><br/>\n",
    "We will only use the \"close\" column of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the return of each day:\n",
    "\n",
    "% Change in a day = $(\\frac{P_2}{P_1}-1) \\times 100\\%$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T11:53:30.073525Z",
     "iopub.status.busy": "2023-01-03T11:53:30.073033Z",
     "iopub.status.idle": "2023-01-03T11:53:30.135332Z",
     "shell.execute_reply": "2023-01-03T11:53:30.13422Z",
     "shell.execute_reply.started": "2023-01-03T11:53:30.073457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Read the data as a pandas DataFrame\n",
    "repsol_stock_raw_data = pd.read_csv(\"/kaggle/input/repsol-stock-data-20-years/RepsolStockData20Years.csv\")\n",
    "#Calculate the percentage change by day\n",
    "returns_repsol = 100 * repsol_stock_raw_data.close.pct_change().dropna()\n",
    "#Drop 0 results, there is a error in the dataset and for 74 days,the stock market was close, so the return is 0\n",
    "returns_repsol = returns_repsol.drop(returns_repsol[returns_repsol == 0].index)\n",
    "#Display the 3 first rows of the Serie\n",
    "returns_repsol.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# 4. EDA: Visualizing Returns & Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained the daily returns for Repsol stock from October 29, 2002 to October 25, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of return column\n",
    "returns_repsol.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the daily % returns is 0.0223%, but we will not use that information, as we will be studying the volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the raw close prices to plot the evolution of the stock\n",
    "close_prices = pd.DataFrame(repsol_stock_raw_data[\"close\"])\n",
    "#By dividing each close price by the first price in our dataset we calculate the accumulated return for each day\n",
    "cum_rets = close_prices / close_prices.iloc[0,:]\n",
    "#Using the plotly.express module we can plot our newly created cum_rets\n",
    "fig = px.line(cum_rets.iloc[:,:], width=1000, height=500)\n",
    "#Adding Title\n",
    "fig.update_layout(title_text='Cumulative Return of Repsol Stock (Not Including Dividends) from 2002-10-28 to 2022-10-28')\n",
    "#This will print the graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Measuring Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The daily volatility is the std of the returns\n",
    "daily_volatility = returns_repsol.std()\n",
    "#The monthly volatility is the result of multiplying the daily vol * square root of 21, this is because there are 21 trading days in a month\n",
    "monthly_volatility = math.sqrt(21) * daily_volatility\n",
    "#The annual volatility is the result of multiplying the daily vol * square root of 252, this is because there are 252 trading days in a year\n",
    "annual_volatility = math.sqrt(252) * daily_volatility\n",
    "#Using tabulate package we can print a nice table\n",
    "print(tabulate([['Repsol',daily_volatility,monthly_volatility,annual_volatility]],headers = ['Daily Volatility %', 'Monthly Volatility %', 'Annual Volatility %'],tablefmt = 'fancy_grid',stralign='center',numalign='center',floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can plot the daily retuns of repsol using a line graph using .plot from pandas\n",
    "returns_repsol.plot(figsize =(16,5), title = 'Repsol daily returns');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a distribution plot using plotly.figure_factory, we reshape the data to have them in a vector\n",
    "return_dist_plot  = ff.create_distplot([returns_repsol.values.reshape(-1)], group_labels = [' '])\n",
    "#We specify the plot layout\n",
    "return_dist_plot.update_layout(showlegend=False, title_text='Distribution of daily Repsol Returns', width=1000, height=500)\n",
    "#Printing the plot\n",
    "return_dist_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code uses the pacf() function from the tsa.stattools module of the statsmodels library (sm) to compute the autocorrelation function.\n",
    "plot_pacf(returns_repsol**2,method=\"yw\")\n",
    "#Print the visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the autocorrelation of squared returns, this positive result will let us to predict volatility using a GARCH model, or either adding the previous volatility as a feature to our XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gar'></a>\n",
    "# 5. GARCH (4,4) vs XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a GARCH model (4,4) that uses a ged distribution\n",
    "model = arch_model(returns_repsol,dist=\"ged\", vol = 'GARCH', p=4, q=4)\n",
    "#Fit the model\n",
    "model_fit = model.fit(disp='off')\n",
    "#Summary of the model\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the full serie as the previosly defined model\n",
    "full_serie_garch = arch_model(returns_repsol,dist=\"ged\", vol = 'GARCH', p=4, q=4)\n",
    "#Fitting the model for the full serie\n",
    "model_fit_full_serie = full_serie_garch.fit(disp='off')\n",
    "#We will plot against the rolling volatility\n",
    "rolling_vol = abs(returns_repsol.rolling(window=22, min_periods=22).std().dropna())\n",
    "#Concatenating the true values, and trained values in our model\n",
    "garch_and_rolling_std = pd.concat([pd.DataFrame(model_fit_full_serie.conditional_volatility),rolling_vol.dropna()], axis=1).dropna()\n",
    "#Plotting it\n",
    "garch_and_rolling_std_plot = px.line(garch_and_rolling_std, title = 'GARCH vs rolling volatility of daily returns TRAIN', width=1000, height=500)\n",
    "#Printing the plot\n",
    "garch_and_rolling_std_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a numeric range of 251 to fill a list of predicted values, for each day we are fitting a new model with the same parameters, but adding the last day.\n",
    "test_size = 251\n",
    "rolling_predictions = []\n",
    "\n",
    "for i in range(test_size):\n",
    "    train = returns_repsol[:-(test_size-i)]\n",
    "    model = arch_model(train,dist=\"ged\", vol = 'GARCH', p=4, q=4)\n",
    "    model_fit = model.fit(disp='off')\n",
    "    pred = model_fit.forecast(horizon=1, reindex = False)\n",
    "    rolling_predictions.append(np.sqrt(pred.variance.values[-1,:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming it to a serie \n",
    "rolling_predictions = pd.Series(rolling_predictions, index= returns_repsol.dropna().index[-test_size:])\n",
    "#Setting plot parameters\n",
    "plt.figure(figsize=(10,4))\n",
    "#True data \n",
    "true, = plt.plot((rolling_vol)[-test_size:])\n",
    "#Predicted data\n",
    "preds, = plt.plot(rolling_predictions)\n",
    "#Plot of the data\n",
    "plt.title('Volatility Prediction for the next 251 Trading Days - Rolling Forecast TEST with GARCH(4,4)', fontsize=20)\n",
    "#Add legend\n",
    "plt.legend(['True Volatility', 'Predicted Volatility'], fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the raw data again, we will be transforming it in a different way\n",
    "repsol_stock_raw_data_for_ml = pd.read_csv(\"/kaggle/input/repsol-stock-data-20-years/RepsolStockData20Years.csv\")\n",
    "#We only need the date and close columns\n",
    "returns_repsol_for_ml = repsol_stock_raw_data_for_ml[[\"date\",\"close\"]]\n",
    "#We change the close column for the percentual change calculation\n",
    "returns_repsol_for_ml[\"close\"] = 100 * repsol_stock_raw_data.close.pct_change().dropna()\n",
    "#Drop 0 returns\n",
    "returns_repsol_for_ml  = returns_repsol_for_ml.drop(returns_repsol_for_ml[returns_repsol_for_ml[\"close\"] == 0].index)\n",
    "#Drop N/A\n",
    "returns_repsol_for_ml = returns_repsol_for_ml.dropna()\n",
    "#As we will be comparing it to the rolling volatility of 22 days, we will transform our target to that\n",
    "returns_repsol_for_ml[\"close\"] = abs(returns_repsol_for_ml[\"close\"].rolling(window=22, min_periods=22).std().dropna())\n",
    "#Convert the date column to datetime format\n",
    "returns_repsol_for_ml[\"date\"] = pd.to_datetime(returns_repsol_for_ml[\"date\"])\n",
    "#Rename the dataframe\n",
    "serie_for_xgboost = returns_repsol_for_ml\n",
    "#Set the test size\n",
    "test_size = 251\n",
    "#Split train and test\n",
    "train_ml = serie_for_xgboost[:-(test_size)].dropna()\n",
    "test_ml = serie_for_xgboost[-(test_size):].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF for extracting features from date\n",
    "def create_features(df, label=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    \n",
    "    \n",
    "    X = df[['dayofweek','quarter','month','year',\n",
    "           'dayofyear','dayofmonth']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the features for the train and test sets\n",
    "X_train, y_train = create_features(train_ml, label=\"close\")\n",
    "X_test, y_test = create_features(test_ml, label=\"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an aditional feature that uses the 4 previous days of rolling volatility, incorporating the autoregressive component to our ML model\n",
    "X_train['prev1']=train_ml['close'].shift(1)\n",
    "X_test['prev1']=test_ml['close'].shift(1)\n",
    "X_train['prev2'] =train_ml['close'].shift(2)\n",
    "X_test['prev2']=test_ml['close'].shift(2)\n",
    "X_train['prev3'] =train_ml['close'].shift(3)\n",
    "X_test['prev3']=test_ml['close'].shift(3)\n",
    "X_train['prev4'] =train_ml['close'].shift(4)\n",
    "X_test['prev4']=test_ml['close'].shift(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and fitting the model\n",
    "reg = xgb.XGBRegressor(n_estimators=1000,early_stopping_rounds=50,)\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of feature importance\n",
    "_ = plot_importance(reg, height=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting with our model for both the train and test data\n",
    "train_ml[\"Predictions\"] = reg.predict(X_train)\n",
    "test_ml['Prediction'] = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataframe with both real and predicted vol\n",
    "XGBoost_and_rolling = pd.concat([pd.DataFrame(list(train_ml[\"Predictions\"]),list(train_ml[\"close\"]))], axis=1).dropna().reset_index()\n",
    "#Renaming columns\n",
    "XGBoost_and_rolling.rename(columns={\"index\":\"Real_Volatility\",0:\"Predicted Volatility\"}, inplace=True)\n",
    "XGBoost_and_rolling.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the predictions of the training data\n",
    "XGBoost_and_rolling = pd.concat([pd.DataFrame(list(train_ml[\"Predictions\"]),list(train_ml[\"close\"]))], axis=1).dropna().reset_index()\n",
    "XGBoost_and_rolling.rename(columns={\"index\":\"Real_Volatility\",0:\"Predicted Volatility\"}, inplace=True)\n",
    "XGBoost_and_rolling = px.line(XGBoost_and_rolling, title = 'XGBOOST vs rolling volatility of daily returns TRAIN', width=1000, height=500)\n",
    "XGBoost_and_rolling.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the predictions for the test data\n",
    "plt.figure(figsize=(10,4))\n",
    "true, = plt.plot(test_ml[\"close\"])\n",
    "preds, = plt.plot(test_ml['Prediction'])\n",
    "plt.title('Volatility Prediction for the next 251 Trading Days - Rolling Forecast TEST with XGBOOST', fontsize=20)\n",
    "plt.legend(['True Returns', 'Predicted Volatility'], fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='res'></a>\n",
    "# 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: \n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Root Mean Square Error, RMSE: Square root of the mean of the difference between\n",
    "the actual data points and the squared prediction value. It penalizes\n",
    "greater or extreme differences more.\n",
    "\n",
    "\n",
    "The RMSE of the model is $RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$.\n",
    "\n",
    "where n is the number of samples, $y_i$ is the true value for the ith sample, and $\\hat{y}_i$ is the predicted value for the ith sample.\n",
    "\n",
    "<br/><br/>\n",
    "Mean Absolute Percentage Error, MAPE: It allows to measure\n",
    "errors relative to the magnitude of the real value.\n",
    "\n",
    "\n",
    "The MAPE of the model is $\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{|A_i - F_i|}{|A_i|}$\n",
    "\n",
    "where $A_i$ is the actual value of the i-th sample, $F_i$ is the forecasted value of the i-th sample, and $n$ is the total number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_Serie = mean_squared_error(garch_and_rolling_std[\"close\"],garch_and_rolling_std[\"cond_vol\"],squared=False)\n",
    "MAPE_Serie = mean_absolute_percentage_error(garch_and_rolling_std[\"close\"], garch_and_rolling_std[\"cond_vol\"])\n",
    "print(f\"The RMSE of our GARCH model in the full serie data is {round(RMSE_Serie,4)}\")\n",
    "print(f\"The MAPE of our GARCH model in the full serie data is {round(MAPE_Serie*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vol = rolling_vol[-test_size:]\n",
    "pred_vol = rolling_predictions\n",
    "RMSE = mean_squared_error(true_vol, pred_vol,squared=False)\n",
    "MAPE = mean_absolute_percentage_error(true_vol, pred_vol)\n",
    "print(f\"The RMSE of our GARCH model in the predicted data is {round(RMSE,4)}\")\n",
    "print(f\"The MAPE of our GARCH model in the predicted data is {round(MAPE*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_Serie_XG = mean_squared_error(train_ml[\"close\"],train_ml[\"Predictions\"],squared=False)\n",
    "MAPE_Serie_XG = mean_absolute_percentage_error(train_ml[\"close\"], train_ml[\"Predictions\"])\n",
    "print(f\"The RMSE of our XGBOOST model in the full serie data is {round(RMSE_Serie_XG,4)}\")\n",
    "print(f\"The MAPE of our XGBOOST model in the full serie data is {round(MAPE_Serie_XG*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vol = test_ml['Prediction']\n",
    "pred_vol = test_ml[\"close\"]\n",
    "RMSE_XG = mean_squared_error(true_vol, pred_vol,squared=False)\n",
    "MAPE_XG = mean_absolute_percentage_error(true_vol, pred_vol)\n",
    "print(f\"The RMSE of our XGBOOST model in the predicted data is {round(RMSE_XG,4)}\")\n",
    "print(f\"The MAPE of our XGBOOST model in the predicted data is {round(MAPE_XG*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate([['MAPE',round(MAPE_Serie*100,2),round(MAPE_Serie_XG*100,2),round(MAPE*100,2),round(MAPE_XG*100,2)]],headers = ['GARCH TRAIN %', 'XGBOOST TRAIN %', 'GARCH PREDICTIONS %','XGBOOST PREDICTIONS %'],tablefmt = 'fancy_grid',stralign='center',numalign='center',floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our study has shown that the GARCH model can effectively predict the monthly rolling volatility of Repsol's stock. However, other modern approaches like XGBoost can improve upon this, as seen in our results where the MAPE was reduced by half to 4.43%. This project showcases the utility of both Machine Learning and GARCH for forecasting stock volatility, and underscores the importance of taking into account the time-varying nature of volatility in financial analysis. While this findings are promising, further research is needed to test these theories across a wider range of stocks, markets, and models in order to reach more general conclusions.\n",
    "\n",
    "I really hope that this notebook was useful and interesting to anyone reading."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2583721,
     "sourceId": 4407536,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
